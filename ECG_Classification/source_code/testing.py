import torch
import torch.nn as nn
from torchvision import transforms
from torch.utils.data import DataLoader, Dataset
from pathlib import Path
from PIL import Image
import os
from sklearn.metrics import classification_report, confusion_matrix, f1_score
import numpy as np
from tqdm import tqdm

from Class_Dataset import MyDataset
from Class_Model import MyModule

###########################    CONFIGURE   #############################

DATA_ROOT_PATH = Path("E:/pv/WORKING/ECG_main_folder/ECG_Classification/data")
MODEL_PATH = Path("E:/pv/WORKING/ECG_main_folder/ECG_Classification/model_save/ecg_model.pth")
BATCH_SIZE = 4

# ====================== Inference & Evaluation ======================

def evaluate():
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # Load model
    model = MyModule()
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.to(device)
    model.eval()

    # Transform
    transform = transforms.Compose([
        transforms.Resize((100, 100)),
        transforms.ToTensor()
    ])

    # Load test data
    test_dataset = MyDataset(root_dir=DATA_ROOT_PATH/'test_images', transform=transform)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    all_preds = []
    all_labels = []

    #L√† ch·∫°y m√¥ h√¨nh tr√™n t·∫≠p test, l·∫•y d·ª± ƒëo√°n (preds) v√† so s√°nh v·ªõi nh√£n th·∫≠t (labels)
    with torch.no_grad(): #T·∫Øt ch·∫ø ƒë·ªô t√≠nh gradient ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ v√† tƒÉng t·ªëc, ko c·∫ßn h·ªçc 
        for inputs, labels in tqdm(test_loader, desc="üîç Testing"): #inputs: batch ·∫£nh scalogram, labels: nh√£n t∆∞∆°ng ·ª©ng (class th·ª±c t·∫ø)
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs) #D·ªØ li·ªáu ƒëi qua m√¥ h√¨nh (N·∫øu m√¥ h√¨nh c√≥ l·ªõp softmax/logits ·ªü cu·ªëi, outputs s·∫Ω l√† [batch_size, num_classes])
            _, preds = torch.max(outputs, 1) #L·∫•y ch·ªâ s·ªë class c√≥ gi√° tr·ªã l·ªõn nh·∫•t ·ªü m·ªói h√†ng ‚Üí ch√≠nh l√† class m√† m√¥ h√¨nh d·ª± ƒëo√°n
                #preds: vector d·ª± ƒëo√°n class (vd: [0, 1, 2, ...])

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            #preds v√† labels ƒë∆∞·ª£c chuy·ªÉn v·ªÅ CPU v√† numpy ƒë·ªÉ: D·ªÖ d√πng v·ªõi sklearn (d√πng t√≠nh accuracy, confusion matrix, F1-score).

    # Evaluation 
    print("‚úÖ Evaluation Result:") #In ra ti√™u ƒë·ªÅ
    print("Confusion Matrix:")
    print(confusion_matrix(all_labels, all_preds))
    print("\nClassification Report:")
    print(classification_report(all_labels, all_preds, digits=4)) #digits=4: l√†m tr√≤n ƒë·∫øn 4 ch·ªØ s·ªë th·∫≠p ph√¢n

    #T√≠nh Macro F1-score cho t·ª´ng l·ªõp, r·ªìi l·∫•y trung b√¨nh c·ªông
    print(f"F1 Score (macro): {f1_score(all_labels, all_preds, average='macro'):.4f}") 

    #Weighted F1: T√≠nh F1-score t·ª´ng l·ªõp, nh∆∞ng c√¢n theo s·ªë l∆∞·ª£ng m·∫´u ·ªü m·ªói l·ªõp
    #Ph√π h·ª£p khi c√°c l·ªõp m·∫•t c√¢n b·∫±ng (unbalanced)
    print(f"F1 Score (weighted): {f1_score(all_labels, all_preds, average='weighted'):.4f}")

if __name__ == '__main__':
    evaluate()
